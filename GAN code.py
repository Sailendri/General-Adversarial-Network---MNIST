## import packages
import torch
import random
import numpy as np
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import sampler
import torchvision.datasets as dset
import os
import numpy.testing as npt
#from torchsummary import summary

import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

## Checks for the availability of GPU 
is_cuda = torch.cuda.is_available()
#is_cuda = False
if is_cuda:
    print("working on gpu!")
else:
    print("No gpu! only cpu ;)")

random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
os.environ['PYTHONHASHSEED'] = '0'

############################################################################### 

import torchvision
import torchvision.transforms as transforms
import os

root = './data/'
if not os.path.isdir(root):
    os.mkdir(root)

train_bs = 128

# Data transformation for the DataLoader - normalizes to between [-1,1]
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])])

training_data = torchvision.datasets.MNIST(root, train=True, transform=transform,download=True)
train_loader = torch.utils.data.DataLoader(dataset=training_data, batch_size=train_bs, shuffle=True, drop_last=True)

def noise(bs, dim):
    """Generate random Gaussian noise vectors N(0,I), with mean 0 and variance 1.
    
    Inputs:
    - bs: integer giving the batch size of noise to generate.
    - dim: integer giving the dimension of the Gaussain noise to generate.
    
    Returns:
    A PyTorch Tensor containing Gaussian noise with shape [bs, dim]
    """
    
    out = (torch.randn((bs, dim)))
    if is_cuda:
        out = out.cuda()
    return out

class Generator(nn.Module):
    def __init__(self, noise_dim=100, out_size=784):
        super(Generator, self).__init__()

        self.layer1 = nn.Linear(noise_dim,256)
        self.layer2 = nn.Linear(256,512)
        self.layer3 = nn.Linear(512,1024)
        self.layer4 = nn.Linear(1024,out_size)
        self.leaky_relu = nn.LeakyReLU(0.2)
        self.tanh = nn.Tanh()

        
    def forward(self, x):
        
        x = self.layer1(x)
        x = self.leaky_relu(x)
        x = self.layer2(x)
        x = self.leaky_relu(x)
        x = self.layer3(x)
        x = self.leaky_relu(x)
        x = self.layer4(x)
        x = self.tanh(x)
        x = x.view(x.size(0),1,28,28)
        return x
    
# Initialize the Generator and move it to GPU (if is_cuda)
generator = Generator()
print(generator)

# move to GPU
if is_cuda:
    generator = generator.cuda()


from torch.nn.modules.flatten import Flatten


class Discriminator(nn.Module):
    def __init__(self, input_size=784):
        super(Discriminator, self).__init__()
        
        self.fc_layer1 = nn.Linear(input_size,512)
        self.leaky_relu = nn.LeakyReLU(0.2)
        self.fc_layer2 = nn.Linear(512,256)
        self.fc_layer3 = nn.Linear(256,1)
        
        
    
    def forward(self, x):
    
        x = x.view(x.size(0),1*28*28)
        x = self.fc_layer1(x)
        x = self.leaky_relu(x)
        x = self.fc_layer2(x)
        x = self.leaky_relu(x)
        x = self.fc_layer3(x)
        y = x.view(x.size(0),1)
        return y      
    
# Initialize the Discriminator and move it to GPU (if is_cuda)
discriminator = Discriminator()

print(discriminator)

if is_cuda:
    discriminator = discriminator.cuda()

# Initialize the 'BCEWithLogitsLoss' object
bce_loss = nn.BCEWithLogitsLoss()

def DLoss(logits_real, logits_fake, targets_real, targets_fake):
    '''
    Returns the Binary Cross Entropy Loss between predictions and targets
    
    Inputs:
        logits_real: the outputs of the discriminator (before the sigmoid) for real images
        logits_fake: the outputs of the discriminator (before the sigmoid) for fake images
        targets_real: groundtruth labels for real images
        targets_fake: groundtruth labels for fake images '''


    logits = torch.cat((logits_real,logits_fake))
    targets = torch.cat((targets_real,targets_fake))
    loss = bce_loss(logits,targets)
    return loss

def GLoss(logits_fake, targets_real):
    '''
    Inputs: 
        logits_fake: Logits from the Discriminator for the fake images generated by the Generator
        targets_real: groundtruth labels (close to 1) for the logits_fake '''
    g_loss = bce_loss(logits_fake,targets_real)
    
    return g_loss

epochs = 41
noise_dim = 100
LR = 0.0002
optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR, betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))

from torch.autograd import backward
## Training loop

for epoch in range(epochs):
    for i, (images, _) in enumerate(train_loader):
        targets_real = (torch.FloatTensor(images.size(0), 1).uniform_(0.8, 1.0))
        targets_fake = (torch.FloatTensor(images.size(0), 1).uniform_(0.0, 0.2))
                
        if is_cuda:
            targets_real = targets_real.cuda()
            targets_fake = targets_fake.cuda()
            images = images.cuda()
        
        ## D-Step
        optimizer_D.zero_grad()
        logits_real = discriminator(images)
        fake_images = generator(noise(train_bs, noise_dim)).detach()
        logits_fake = discriminator(fake_images)
        d_loss = DLoss(logits_real, logits_fake, targets_real, targets_fake)
        d_loss.backward()
        optimizer_D.step()

        ## G-Step
        optimizer_G.zero_grad()
        #logits_real = discriminator(images)
        fake_images = generator(noise(train_bs, noise_dim))
        logits_fake = discriminator(fake_images)
        g_loss = GLoss(logits_fake, targets_real)
        g_loss.backward()
        optimizer_G.step()



    print("Epoch:  ", epoch)
    print("D Loss: ", d_loss.item())
    print("G Loss: ", g_loss.item())
          
    if epoch % 2 == 0:
        viz_batch = fake_images.data.cpu().numpy()
        viz_batch = viz_batch[:100,:,:,:]
        viz_batch = viz_batch.reshape(-1,28*28).squeeze()
        viz_batch = viz_batch.reshape(10,10, 28,28).transpose(0,2,1,3).reshape(28*10,-1)

        plt.figure(figsize = (8,8))
        plt.axis('off')
        plt.imshow(viz_batch, cmap='gray')
        plt.show()